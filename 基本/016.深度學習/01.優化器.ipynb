{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  優化器\n",
    "常見的優化器\n",
    "*   Batch Gradient Descent （BGD, 批次梯度下降）\n",
    "*   Stochastic Gradient Descent (SGD, 隨機梯度下降)\n",
    "*   Mini-Batch Gradient Descent （MBGD, 小批次梯度下降）\n",
    "\n",
    "[參考資料](https://www.796t.com/content/1545433422.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Gradient Descent （BGD, 批次梯度下降）\n",
    "*   梯度更新規則:\\\n",
    "    <font color = red>BGD 採用整個訓練集</font>的資料來計算 cost function 對引數的梯度：$\\theta = \\theta - \\eta \\cdot \\nabla_\\theta J(\\theta)$\n",
    "\n",
    "*   缺點:\\\n",
    "    由於這種方法是在一次更新中，就<font color = red>對整個資料集計算梯度，所以計算起來非常慢</font>，遇到很大量的資料集也會非常棘手，而且不能投入新資料實時更新模型。\n",
    "\n",
    "\n",
    "```python\n",
    "for i in range(nb_epochs):\n",
    "  params_grad = evaluate_gradient(loss_function, data, params)\n",
    "  params = params - learning_rate * params_grad\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent (SGD, 隨機梯度下降)\n",
    "*   梯度更新規則:\\\n",
    "    和 BGD 的一次用所有資料計算梯度相比，<font color = red>SGD 每次更新時對每個樣本進行梯度更新</font>，對於很大的資料集來說，可能會有相似的樣本，這樣 BGD 在計算梯度時會出現冗餘，而 SGD 一次只進行一次更新，就沒有冗餘，而且比較快，並且可以新增樣本。\\\n",
    "    $\\theta = \\theta - \\eta \\cdot \\nabla_\\theta J(\\theta;x^{(i)};y^{(i)})$\n",
    "  \n",
    "* 缺點：\\\n",
    "  SGD 因為<font color = red>更新比較頻繁，會造成 cost function 有嚴重的震盪</font>。\\\n",
    "  BGD 可以收斂到區域性極小值，當然 SGD 的震盪可能會跳到更好的區域性極小值處。\\\n",
    "  當我們稍微減小 learning rate，SGD 和 BGD 的收斂性是一樣的。\n",
    "\n",
    "```python\n",
    "for i in range(nb_epochs):\n",
    "  np.random.shuffle(data)\n",
    "  for example in data:\n",
    "    params_grad = evaluate_gradient(loss_function, example, params)\n",
    "    params = params - learning_rate * params_grad\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-Batch Gradient Descent （MBGD, 小批次梯度下降）\n",
    "*   梯度更新規則：\\\n",
    "    MBGD <font color = red>每一次利用一小批樣本</font>，即 n 個樣本進行計算，這樣它可以降低引數更新時的方差，收斂更穩定，另一方面可以充分地利用深度學習庫中高度優化的矩陣操作來進行更有效的梯度計算。\\\n",
    "    $\\theta = \\theta - \\eta \\cdot \\nabla_\\theta J(\\theta;x^{(i:i+n)};y^{(i:i+n)})$\n",
    "\n",
    "*   缺點：（兩大缺點）\n",
    "\n",
    "  1.  Mini-batch gradient descent 不能保證很好的收斂性，<font color = red>learning rate(學習率) 如果選擇的太小，收斂速度會很慢，如果太大，loss function 就會在極小值處不停地震盪甚至偏離</font>。（有一種措施是先設定大一點的學習率，當兩次迭代之間的變化低於某個閾值後，就減小 learning rate，不過這個閾值的設定需要提前寫好，這樣的話就不能夠適應資料集的特點。）對於非凸函式，還要避免陷於區域性極小值處，或者鞍點處，因為鞍點周圍的error是一樣的，所有維度的梯度都接近於0，SGD 很容易被困在這裡。（會在鞍點或者區域性最小點震盪跳動，因為在此點處，如果是訓練集全集帶入即BGD，則優化會停止不動，如果是mini-batch或者SGD，每次找到的梯度都是不同的，就會發生震盪，來回跳動。）\n",
    "  \n",
    "  2.  SGD對所有引數更新時應用同樣的 learning rate，如果我們的資料是稀疏的，我們更希望對出現頻率低的特徵進行大一點的更新。LR會隨著更新的次數逐漸變小。\n",
    "\n",
    "```python\n",
    "#和 SGD 的區別是每一次迴圈不是作用於每個樣本，而是具有 n 個樣本的批次。\n",
    "#超引數設定值:  n 一般取值在 50～256\n",
    "for i in range(nb_epochs):\n",
    "  np.random.shuffle(data)\n",
    "  for batch in get_batches(data, batch_size=50):\n",
    "    params_grad = evaluate_gradient(loss_function, batch, params)\n",
    "    params = params - learning_rate * params_grad\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4206480afc6f8f20a91f9b30d4e9d2907612993729e8a589e2c16a111e42b69c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
