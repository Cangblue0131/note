<!-- markdownlint-disable MD033 -->
<!-- markdownlint-disable MD010 -->
<!-- markdownlint-disable MD037 -->
<!-- markdownlint-disable MD041 -->
<!-- markdownlint-disable MD024 -->

# L12 Generalized Linear Model

從此開始 Y 可能是離散型資料, 例如 Y = {0,1} 或 count data : {0, 1, 2, ...}

過去目標是得到 $Y = E[Y|X] + \epsilon$, 其中 $E[Y|X] = \beta_0 + \beta^TX$, $Y \sim N(E[Y|X], \sigma^2)$. 不過若 Y 為離散型, 則此結果會很奇怪.

當 Y 為 {0, 1} 時, $E[Y|X] = P(Y=1|X) \in [0, 1]$; 當 Y 為 {0, 1, ....}時, $E[Y|X]$ 也 $\in [0, 1]$

## 此時會用到的分佈

### Bernoulli

$Y~\sim Ber(p)$ with $p = P(Y=1)$.

* $f(y) = p^y(1-p)^{(1-y)},~ y = 0, 1$
* $E(Y) = p$
* $Var(Y) = p(1-p)$, 所以 p = 0.5 時變異最大.

### Poission

$Y \sim Poi(\lambda)$

* $f(y) = \frac{e^{-\lambda}\lambda^y}{y!}$
* $E(Y) = \lambda$
* $Var(Y) = \lambda$

## Estimation of parameter

由 $Y_i \sim(iid) Ber(p)$ 來說

### likehood function

從假設給定 p, 求得 "看到目前資料的機率去推論" 最有可能的p.

看到目前資料的機率 $Y_i \to f_p(Y_i)$, 所以看到整組資料的機率為 $\prod f_p(Y_i) = L(p)$.

其值可畫出圖形類似如下:

![L12-1](figure\L12-1.PNG)

最高點代表是 該p的情況下 此資料最有可能出現, 找這點的估計方法稱為 MLE(最大概似估計, maximum likelihood estimation).

從式子可知道, $L(p) = \prod(p^{Y_i}(1-p)^{(1-Y_i)})$. 使用 log 和 微分最後推得出 $\hat{p} = \frac{1}{n}\sum Y_i$

## Inference rocedure

Model : $Y \sim f_\theta(y)$

Data : ${Y_i}$

Aim : estimate $\theta$ $\to$ 使用 MLE 估計.

# L13 Generalized Linear Model (2)

## Logistic Regression

* Binary response $Y \in \{0, 1\}$
* covariates : $X = (X_1, ..., X_p)$
* target : $E[Y|X] = P(Y=1|X) = P_x$

### Modeling

1. $Y|X \sim Bernoulli(P_x)$
2. $g(P_x) = \beta + \beta^TX$, g 稱為連結函數, 用來連結條件機率和線性函數.

* odds : $\frac{P_x}{1-P_x} \in (0, \infty)$
* log-odds : $ln(\frac{P_x}{1-P_x}) \in (-\infty, \infty)$

因為 log_odds 可以把 $P_x$ 轉換成在實數範圍內的結果, 所以模型使用 log-odds 來當連結函數.所以 

$g(P_x) = ln(\frac{P_x}{1-P_x}) = \beta_0 + \beta^TX$

可以得到,

$P(Y=1|X) = \frac{exp(\beta_0 + \beta^TX)}{1+exp(\beta_0 + \beta^TX)} \in [0,1]$

### Meaning of $\beta_1$

解釋 $\beta$ 的意義, 以 $\beta_1$ 為例.

從機率上來看, 若 $X_1$ 改變一單位, 用以下式子來看改變

$\frac{P(Y=1|X_1=X_1+1,...)}{1-P(Y=1|X_1=X_1+1,...)} / \frac{P(Y=1|X_1=X_1,...)}{1-P(Y=1|X_1=X_1,...)}$

已知 $\frac{P_x}{1-P_x} = exp(\beta_0 + \beta^TX)$

所以上面式子可以變成:

$exp(\beta_0 + \beta_1(X_1+1)+...)$ 和 $exp(\beta_0 + \beta_1(X_1)+...)$

兩個相除 $= e^{\beta_1}$

換句話說, $\beta_1$ 代表的意義為, 當 X_1 增加一單位時, odds 翻了 $exp(\beta_1)$ 倍.

從此結論可知, 假設 $\beta_1>0, \to \exp{\beta_1} > 1$. 也就是說 $odds(x_1 = X+1) > odds(x_1 = x)$.

又因odd的值和機率是一對一的, 所以:
$P(Y=1|X_1=x+1, ....) > P(Y=1|X_1=x, ....)$

所以從 $\beta$ 可以看出該變數是'危險因子'還是'保護因子'.

### 圖形參考(假設解釋變數只有1個)
![L12-1](figure\L13-1.PNG)


模型可以寫開成 : $ln(\frac{p_x}{1-p_x}) = \beta_0 + \beta^TX$, 這邊後面 $\beta$ 部分, <font color = red>和前面的線性回歸一樣, 可以放入虛擬變數或是交互作用項.</font>

另外, 單個參數和多個, 其得到的 $\beta$ 也不同(係數會不同). 並且在<font color = red>多個參數下, 如果係數為0不代表沒有解釋力, 是因為在其他變數存在的情況下沒有貢獻. </font> 這部分結論都和前面一樣.

### 參數相關

在 logis. reg., 如果有 p 個特徵則要估計 p+1 個參數(在一般線性回歸有 P+2個, 多一個 $\sigma^2$).

p+1個分別為 : $\beta_i,  i \in \{0, 1, ..., p\}$

資料 : $\{Y_i, X_i\}^n_{i=1}$

在給定X的情況下, 每個觀察值的機率為 $P_{X_i}^{Yi}(1-P_{X_i})^{Y_i}$, 這裡 $P_{X_i}= \frac{exp(\beta_0 + \beta^TX}{1+exp(\beta_0 + \beta^TX)}$.

從這可以得到, Likelihood func. : $L(\theta) = \prod{P_{X_i}^{Yi}(1-P_{X_i})^{Y_i}}$. 所以 MLE :$\theta=argmax_\theta L(\theta)$
