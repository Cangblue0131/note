<!-- markdownlint-disable MD033 -->
<!-- markdownlint-disable MD010 -->
<!-- markdownlint-disable MD037 -->
<!-- markdownlint-disable MD041 -->
<!-- markdownlint-disable MD024 -->

補 :

關於 mean response 和 預測 的差異可以去聽這段 : [L14](https://youtu.be/8Qzqf51O6ZE?si=2OPhPTORPz05FFLA&t=1308) 

# L12 Generalized Linear Model

從此開始 Y 可能是離散型資料, 例如 Y = {0,1} 或 count data : {0, 1, 2, ...}

過去目標是得到 $Y = E[Y|X] + \epsilon$, 其中 $E[Y|X] = \beta_0 + \beta^TX$, $Y \sim N(E[Y|X], \sigma^2)$. 不過若 Y 為離散型, 則此結果會很奇怪.

當 Y 為 {0, 1} 時, $E[Y|X] = P(Y=1|X) \in [0, 1]$; 當 Y 為 {0, 1, ....}時, $E[Y|X]$ 也 $\in [0, 1]$

## 此時會用到的分佈

### Bernoulli

$Y~\sim Ber(p)$ with $p = P(Y=1)$.

* $f(y) = p^y(1-p)^{(1-y)},~ y = 0, 1$
* $E(Y) = p$
* $Var(Y) = p(1-p)$, 所以 p = 0.5 時變異最大.

### Poission

$Y \sim Poi(\lambda)$

* $f(y) = \frac{e^{-\lambda}\lambda^y}{y!}$
* $E(Y) = \lambda$
* $Var(Y) = \lambda$

## Estimation of parameter

由 $Y_i \sim(iid) Ber(p)$ 來說

### likehood function

從假設給定 p, 求得 "看到目前資料的機率去推論" 最有可能的p.

看到目前資料的機率 $Y_i \to f_p(Y_i)$, 所以看到整組資料的機率為 $\prod f_p(Y_i) = L(p)$.

其值可畫出圖形類似如下:

![L12-1](figure\L12-1.PNG)

最高點代表是 該p的情況下 此資料最有可能出現, 找這點的估計方法稱為 MLE(最大概似估計, maximum likelihood estimation).

從式子可知道, $L(p) = \prod(p^{Y_i}(1-p)^{(1-Y_i)})$. 使用 log 和 微分最後推得出 $\hat{p} = \frac{1}{n}\sum Y_i$

## Inference rocedure

Model : $Y \sim f_\theta(y)$

Data : ${Y_i}$

Aim : estimate $\theta$ $\to$ 使用 MLE 估計.

# L13 Generalized Linear Model (2)

## Logistic Regression

* Binary response $Y \in \{0, 1\}$
* covariates : $X = (X_1, ..., X_p)$
* target : $E[Y|X] = P(Y=1|X) = P_x$

### Modeling

1. $Y|X \sim Bernoulli(P_x)$
2. $g(P_x) = \beta + \beta^TX$, g 稱為連結函數, 用來連結條件機率和線性函數.

* odds : $\frac{P_x}{1-P_x} \in (0, \infty)$
* log-odds : $ln(\frac{P_x}{1-P_x}) \in (-\infty, \infty)$

因為 log_odds 可以把 $P_x$ 轉換成在實數範圍內的結果, 所以模型使用 log-odds 來當連結函數.所以 

$g(P_x) = ln(\frac{P_x}{1-P_x}) = \beta_0 + \beta^TX$

可以得到,

$P(Y=1|X) = \frac{exp(\beta_0 + \beta^TX)}{1+exp(\beta_0 + \beta^TX)} \in [0,1]$

### Meaning of $\beta_1$

解釋 $\beta$ 的意義, 以 $\beta_1$ 為例.

從機率上來看, 若 $X_1$ 改變一單位, 用以下式子來看改變

$\frac{P(Y=1|X_1=X_1+1,...)}{1-P(Y=1|X_1=X_1+1,...)} / \frac{P(Y=1|X_1=X_1,...)}{1-P(Y=1|X_1=X_1,...)}$

已知 $\frac{P_x}{1-P_x} = exp(\beta_0 + \beta^TX)$

所以上面式子可以變成:

$exp(\beta_0 + \beta_1(X_1+1)+...)$ 和 $exp(\beta_0 + \beta_1(X_1)+...)$

兩個相除 $= e^{\beta_1}$

換句話說, $\beta_1$ 代表的意義為, 當 X_1 增加一單位時, odds 翻了 $exp(\beta_1)$ 倍.

從此結論可知, 假設 $\beta_1>0, \to \exp{\beta_1} > 1$. 也就是說 $odds(x_1 = X+1) > odds(x_1 = x)$.

又因odd的值和機率是一對一的, 所以:
$P(Y=1|X_1=x+1, ....) > P(Y=1|X_1=x, ....)$

所以從 $\beta$ 可以看出該變數是'危險因子'還是'保護因子'.

### 圖形參考(假設解釋變數只有1個)
![L12-1](figure\L13-1.PNG)


模型可以寫開成 : $ln(\frac{p_x}{1-p_x}) = \beta_0 + \beta^TX$, 這邊後面 $\beta$ 部分, <font color = red>和前面的線性回歸一樣, 可以放入虛擬變數或是交互作用項.</font>

另外, 單個參數和多個, 其得到的 $\beta$ 也不同(係數會不同). 並且在<font color = red>多個參數下, 如果係數為0不代表沒有解釋力, 是因為在其他變數存在的情況下沒有貢獻. </font> 這部分結論都和前面一樣.

### 參數相關

在 logis. reg., 如果有 p 個特徵則要估計 p+1 個參數(在一般線性回歸有 P+2個, 多一個 $\sigma^2$).

p+1個分別為 : $\beta_i,  i \in \{0, 1, ..., p\}$

資料 : $\{Y_i, X_i\}^n_{i=1}$

在給定X的情況下, 每個觀察值的機率為 $P_{X_i}^{Yi}(1-P_{X_i})^{Y_i}$, 這裡 $P_{X_i}= \frac{exp(\beta_0 + \beta^TX}{1+exp(\beta_0 + \beta^TX)}$.

從這可以得到, Likelihood func. : $L(\theta) = \prod{P_{X_i}^{Yi}(1-P_{X_i})^{Y_i}}$. 所以 MLE :$\theta=argmax_\theta L(\theta)$

# L14 Generalized Linear Model (3)

### Confidence Interval of $B_j$

$\hat{\beta}_j \pm \sqrt{Var(\hat{\beta}_j)}Z_{\frac{\alpha}{2}}$

### Testing

* $H_0 : \beta_j = \beta_j^*$

$Z = \frac{\hat{\beta}_j - \beta_j^*}{\sqrt{\hat{\beta}_j}} \sim N(0,1)$

## Likelihood Ratio Test (LRT)

類似 F test 的延伸, 用於檢定多個特徵是否為0.

$H_0 : \beta_{k+1} = ... = \beta_p = 0$

* Full model : 全部的特徵做的模型, $g(P_x) = \sum^p\beta_iX_i$
* Reduced model : 移除懷疑為0的特徵後的模型, $g(P_x) = \sum^k\beta_iX_i$

簡單來說, 兩個 Likelihood 得到的值, 若越接近則越不容易拒絕 $H_0$.
也就是 $T = \frac{L_f(\hat{\theta}_f)}{L_r(\hat{\theta}_r)}$ 越大, 越容易拒絕. 其中 $2lnT \sim X^2_{p-k}$.

這之中, 若 k = 1 代表檢定所有的特徵是否都無意義.

[詳細說明點這裡](https://youtu.be/8Qzqf51O6ZE?si=JfDDsnnhMtXkRnEr&t=635)

## Estimation of mean response

$\hat{P}_x = \frac{exp(\hat{\beta_0} + \hat{\beta}^T X)}{1 + exp(\hat{\beta_0} + \hat{\beta}^T X)} \in [0,1]$

因為 Y 為 0 or 1, 所以 Y 不能直接用 mean response 去當. 所以在 log. reg. 預測跟估計是不同的. 這實是去針對預測值去做二分類, 也就是

* $\hat{Y}_0 = 0 ~~ if~ \hat{P}_{X_0} \leq C ~else~ \hat{Y}_0 = 1$

其中, $\hat{P}_{X} > C \Longleftrightarrow \hat{\beta}_0 + \hat{\beta}^TX > ln\frac{c}{1-c}$

由此可畫出圖形類似如下:

![L14-1](figure\L14-1.PNG)

由此可知, 雖然預測時 mean response 是曲線的, 但在此做的分類是線性的, 也就是 Linear classifier.

