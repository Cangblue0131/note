{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 回歸模型\n",
    "\n",
    "用於預測連續數值的模型.\n",
    "\n",
    "* 羅吉斯回歸是以連續模型的結果為基礎再去做分類, 所以實際上也是個回歸模型."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn.linear_model\n",
    "\n",
    "1. Linear Regression : \n",
    "\n",
    "    * 線性回歸是一種用於建立線性模型的方法，通常用於預測數值型目標變量。\n",
    "    * 參數：fit_intercept（是否計算截距，預設為 True）\n",
    "    * 超參數：無\n",
    "    * 可用於 polynomial regression, 需使用 PolynomialFeatures(degree = n) 創造參數\n",
    "\n",
    "```python\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    # 多項式回歸\n",
    "    X_poly = df[['X1']]\n",
    "    y_poly = df['y']\n",
    "    poly_features = PolynomialFeatures(degree=2)  # 二次多項式\n",
    "    X_poly_transformed = poly_features.fit_transform(X_poly)\n",
    "    reg_poly = LinearRegression()\n",
    "    reg_poly.fit(X_poly_transformed, y_poly)\n",
    "```\n",
    "\n",
    "---\n",
    "2. Ridge Regression:\n",
    "\n",
    "    * Ridge 回歸是一種線性回歸的擴展，它使用 L2 正則化來控制模型的複雜度，以防止過度擬合。\n",
    "    * 參數：alpha（正則化參數，預設為 1.0），fit_intercept（是否計算截距，預設為 True）\n",
    "    * 超參數：alpha（正則化參數，用於控制模型的複雜度）\n",
    "\n",
    "---\n",
    "3. Lasso Regression:\n",
    "\n",
    "    * Lasso 回歸是一種線性回歸的擴展，它使用 L1 正則化來控制模型的複雜度，同時具有特徵選擇的效果。\n",
    "    * 參數：alpha（正則化參數，預設為 1.0），fit_intercept（是否計算截距，預設為 True）\n",
    "    * 超參數：alpha（正則化參數，用於控制模型的複雜度）\n",
    "\n",
    "---\n",
    "4. Logistic Regression:\n",
    "\n",
    "    * 邏輯回歸是一種用於分類問題的線性模型，它通常用於預測二元或多元目標變量的概率。\n",
    "    * 參數：C（正則化參數的倒數，預設為 1.0），fit_intercept（是否計算截距，預設為 True）, solver (模型中用於優化參數的演算法選項)\n",
    "    * solver = liblinear: 這是一個內置的優化算法，適用於小型數據集。它使用坐標下降法和拟牛頓法（Limited-memory Broyden-Fletcher-Goldfarb-Shanno algorithm，L-BFGS）來優化參數，適合二元分類或多類別分類問題。\n",
    "    * solver = lbfgs: 這也是一個內置的優化算法，適用於較大的數據集。它使用拟牛頓法來優化參數，對於多類別分類問題也是有效的。\n",
    "    * solver = saga: 這是一個支持L1和L2正則化的優化算法，適用於大型數據集。它使用序列最小角回歸算法（Sequential Minimal Optimization algorithm，SMO）來優化參數。\n",
    "    * 超參數：C（正則化參數，用於控制模型的複雜度）\n",
    "\n",
    "---\n",
    "5. ElasticNet Regression:\n",
    "\n",
    "    * ElasticNet 回歸是 Ridge 和 Lasso 回歸的結合，它同時使用 L1 和 L2 正則化來控制模型的複雜度，可以在具有高度相關特徵的數據集上取得良好的效果。\n",
    "    * $\\frac{1}{2n}\\sum({y_i - \\hat{y})}^2 + \\alpha(l1_{ratio}\\sum{|\\beta_j|} + \\frac{1}{2}(1-l1_{ratio})\\sum{\\beta_j^2})$\n",
    "    * 式子和一般熟悉的不同, 是為了平衡L1和L2正則化之間的比例.\n",
    "    * 參數：alpha（正則化參數，預設為 1.0），l1_ratio（L1 正則化在綜合正則化中的比例，預設為 0.5），fit_intercept（是否計算截距，預設為 True）\n",
    "    * 超參數：alpha（正則化參數，用於控制模型的複雜度），l1_ratio（L1 正則化在綜合正則化中的比例，用於控制 L1 和 L2 正則化的相對比例）\n",
    "\n",
    "---\n",
    "6. PassiveAggressiveRegressor:\n",
    "\n",
    "    * PassiveAggressive 回歸是一種適用於線性回歸的算法，它可以處理大量數據，並在線上學習過程中進行模型的更新。\n",
    "    * 參數：C（正則化參數，預設為 1.0），fit_intercept（是否計算截距，預設為 True）\n",
    "    * 超參數：C（正則化參數，用於控制模型的複雜度）\n",
    "\n",
    "---\n",
    "7. BayesianRidge:\n",
    "\n",
    "    * BayesianRidge 回歸是一種貝葉斯線性回歸模型，它通過引入先驗分佈來估計模型的參數，並通過最大後驗概率（MAP）估計來進行回歸分析。\n",
    "    * 參數：alpha_1（L1 正則化的精度，預設為 1e-6），alpha_2（L2 正則化的精度，預設為 1e-6），lambda_1（截距的精度，預設為 1e-6），lambda_2（斜率的精度，預設為 1e-6），fit_intercept（是否計算截距，預設為 True）\n",
    "    * 超參數：alpha_1（L1 正則化的精度，用於控制 L1 正則化的強度），alpha_2（L2 正則化的精度，用於控制 L2 正則化的強度），lambda_1（截距的精度，用於控制截距的估計精度），lambda_2（斜率的精度，用於控制斜率的估計精度）\n",
    "\n",
    "---\n",
    "8. HuberRegressor:\n",
    "\n",
    "    * HuberRegressor 回歸是一種對異常值具有魯棒性的回歸算法，它使用 Huber 損失函數來平衡平方損失和絕對損失。\n",
    "    * 參數：epsilon（Huber 損失的閾值，預設為 1.35），fit_intercept（是否計算截距，預設為 True）\n",
    "    * 超參數：epsilon（Huber 損失的閾值，用於控制對異常值的敏感度）\n",
    "\n",
    "---\n",
    "9. RANSACRegressor:\n",
    "\n",
    "    * RANSACRegressor 回歸是一種穩健(魯棒)性回歸算法，它通過隨機抽樣子集來擬合模型，以應對包含異常值的數據。\n",
    "    * 參數：min_samples（樣本最小值，預設為 None），residual_threshold（殘差閾值，預設為 None），is_data_valid（樣本是否有效的函數，預設為 None），max_trials（最大迭代次數，預設為 100），fit_intercept（是否計算截距，預設為 True）\n",
    "    * 超參數：無\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn.linear_model 以外的迴歸\n",
    "\n",
    "1. 支持向量回歸(SVR)\n",
    "\n",
    "    * 它通過在特徵空間中找到一條線（對於線性SVR）或一個超平面（對於非線性SVR）來進行回歸預測。\n",
    "    ```python\n",
    "        from sklearn.svm import SVR\n",
    "        SVR(kernel='linear', C=1.0, epsilon=0.1)\n",
    "    ```\n",
    "    * 參數 : \n",
    "    * kernel：用於指定SVM的核函數。可以是'linear'（線性核函數）、'poly'（多項式核函數）、'rbf'（高斯核函數）等。默認是'rbf'。\n",
    "    * 超參數 :\n",
    "    * C：用於控制錯誤項的懲罰參數。較小的C值表示容忍較多的錯誤，較大的C值表示更嚴格地避免錯誤。默認值為1.0。\n",
    "    * epsilon：SVR中的ε，也稱為ε-tube。它用於確定损失函數中允許的預測值和實際值之間的誤差。默認值為0.1。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### python\n",
    "\n",
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######\n",
    "## 1. 獲得資料並把資料分為 training 和 test\n",
    "#######\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# 載入數據集\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target # 注: 這邊y的變量有3個\n",
    "\n",
    "# 將數據集分成訓練集和測試集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression 最佳超參數組合: {'C': 1, 'solver': 'saga'}\n",
      "Logistic Regression 交叉驗證: 0.975\n"
     ]
    }
   ],
   "source": [
    "#######\n",
    "## 2. 使用GridSearchCV來尋找最佳的超參數組合\n",
    "#######\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "\n",
    "# 忽略警告訊息\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 建立Logistic Regression模型\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# 定義超參數網格，這裡我們調整正則化參數C和solver\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "              'solver': ['liblinear', 'lbfgs', 'saga']}\n",
    "\n",
    "# 使用5折交叉驗證進行網格搜索\n",
    "grid_search = GridSearchCV(logistic_model, param_grid, cv=5)\n",
    "\n",
    "# 訓練模型\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 打印最佳超參數組合和交叉驗證得分\n",
    "print(\"Logistic Regression 最佳超參數組合:\", grid_search.best_params_)\n",
    "print(\"Logistic Regression 交叉驗證:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最終模型的準確率: 1.0\n"
     ]
    }
   ],
   "source": [
    "# 使用最佳超參數組合來建立最終的Logistic Regression模型\n",
    "best_logistic_model = LogisticRegression(C=grid_search.best_params_['C'], solver=grid_search.best_params_['solver'])\n",
    "\n",
    "# 訓練最終的模型\n",
    "best_logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# 使用測試集來評估模型的性能\n",
    "accuracy = best_logistic_model.score(X_test, y_test)\n",
    "print(\"最終模型的準確率:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
