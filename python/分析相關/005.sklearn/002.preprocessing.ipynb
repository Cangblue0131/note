{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn.preprocessing\n",
    "\n",
    "用於 處理缺失值 或 data的調整等."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 處理缺失值\n",
    "\n",
    "1. SimpleImputer：用於填補缺失值的類別。可以使用均值、中位數、最常見值等方法進行填補。\n",
    "\n",
    "    * strategy：指定填補缺失值的策略，可以是以下幾種之一：\n",
    "        * \"mean\"：使用特徵的均值填補缺失值。\n",
    "        * \"median\"：使用特徵的中位數填補缺失值。\n",
    "        * \"most_frequent\"：使用特徵的最常見值填補缺失值。\n",
    "        * \"constant\"：使用指定的常數值填補缺失值（需要指定fill_value參數）。\n",
    "    * fill_value：當strategy為\"constant\"時，指定要填補的常數值。\n",
    "\n",
    "---\n",
    "2. KNNImputer：用於使用k最近鄰居算法填補缺失值。\n",
    "\n",
    "    * n_neighbors：指定k最近鄰居算法中的k值，即填補缺失值時要考慮的鄰居數量。\n",
    "    * weights：指定k最近鄰居算法中的權重方式，可以是以下幾種之一：\n",
    "        * \"uniform\"：所有鄰居的權重相同。\n",
    "        * \"distance\"：鄰居的權重與其距離成反比。\n",
    "    * metric：指定計算鄰居距離的度量方式，例如\"euclidean\"、\"manhattan\"等。\n",
    "\n",
    "---\n",
    "3. IterativeImputer：用於使用迭代算法填補缺失值。\n",
    "\n",
    "    * estimator：指定用於估計缺失值的機器學習模型，可以是任何支持fit和predict方法的模型。\n",
    "    * max_iter：指定迭代的最大次數。\n",
    "    * initial_strategy：指定初始的填補策略，可以是\"mean\"、\"median\"、\"most_frequent\"或者\"constant\"。\n",
    "    * imputation_order：指定填補缺失值的順序，可以是\"ascending\"（從第一個特徵開始填補）或\"descending\"（從最後一個特徵開始填補）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### python\n",
    "\n",
    "<font size = 5>處理缺失值</font>\n",
    "\n",
    "<font size = 5>SimpleImputer 的例子：</font>\n",
    "\n",
    "1. 使用 DataFrame.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.  2.  7.5]\n",
      " [4.  5.  6. ]\n",
      " [7.  8.  9. ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# 假设有一个带有缺失值的 DataFrame\n",
    "data = pd.DataFrame({'A': [1, 4, 7],\n",
    "                     'B': [2, np.nan, 8],\n",
    "                     'C': [np.nan, 6, 9]})\n",
    "\n",
    "# 将 DataFrame 转换为 NumPy 数组\n",
    "data_array = data.values\n",
    "\n",
    "# 建立 SimpleImputer 对象，使用均值填补缺失值\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# 对数据进行填补\n",
    "imputed_data_array = imputer.fit_transform(data_array)\n",
    "\n",
    "print(imputed_data_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 使用 apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B    C\n",
      "0  1.0  2.0  7.5\n",
      "1  4.0  5.0  6.0\n",
      "2  7.0  8.0  9.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# 假设有一个带有缺失值的 DataFrame\n",
    "data = pd.DataFrame({'A': [1, 4, 7],\n",
    "                     'B': [2, np.nan, 8],\n",
    "                     'C': [np.nan, 6, 9]})\n",
    "\n",
    "# 建立 SimpleImputer 对象，使用均值填补缺失值\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# 对 DataFrame 的每一列应用填充方法\n",
    "imputed_data = data.apply(lambda col: imputer.fit_transform(col.values.reshape(-1, 1)).flatten())\n",
    "\n",
    "print(imputed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5>IterativeImputer 的例子：</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         2.         3.00203009]\n",
      " [4.         4.99796991 6.        ]\n",
      " [7.         8.         9.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# 假設有一個帶有缺失值的數據\n",
    "data = np.array([[1, 2, np.nan],\n",
    "                 [4, np.nan, 6],\n",
    "                 [7, 8, 9]])\n",
    "\n",
    "# 建立 IterativeImputer 對象，使用線性回歸模型估計缺失值\n",
    "imputer = IterativeImputer(estimator=LinearRegression())\n",
    "\n",
    "# 對數據進行填補\n",
    "imputed_data = imputer.fit_transform(data)\n",
    "\n",
    "print(imputed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data 調整\n",
    "\n",
    "1. StandardScaler: 將特徵縮放成標準正態分佈，即平均值為0，標準差為1。\n",
    "\n",
    "    * 參數：\n",
    "    * with_mean: bool, 默認為True，是否將特徵縮放成平均值為0。\n",
    "    * with_std: bool, 默認為True，是否將特徵縮放成標準差為1。\n",
    "\n",
    "---\n",
    "2. MinMaxScaler: 將特徵縮放到給定的最小值和最大值之間。\n",
    "\n",
    "    * 參數：\n",
    "    * feature_range: tuple, 默認為(0, 1)，指定特徵的最小值和最大值。\n",
    "\n",
    "---\n",
    "3. RobustScaler: 將特徵縮放成中位數和四分位距的比例，能夠處理離群值。\n",
    "\n",
    "    * 參數：\n",
    "    * with_centering: bool, 默認為True，是否將特徵縮放成中位數為0。\n",
    "    * with_scaling: bool, 默認為True，是否將特徵縮放成四分位距為1。\n",
    "\n",
    "---\n",
    "4. LabelEncoder: 將目標變數（類別）轉換成數字標籤。\n",
    "\n",
    "    * 參數：無。\n",
    "\n",
    "---\n",
    "5. OneHotEncoder: 將目標變數（類別）轉換成二進制的稀疏矩陣表示。\n",
    "\n",
    "    * 參數：\n",
    "    * drop: str, 默認為None，指定是否刪除一個特徵類別，以避免共線性。\n",
    "\n",
    "---\n",
    "6. PolynomialFeatures: 創建原特徵的高次多項式特徵。\n",
    "\n",
    "    * 參數：\n",
    "    * degree: int, 默認為2，指定多項式的次數。\n",
    "\n",
    "---\n",
    "7. KBinsDiscretizer: 將連續特徵切割成指定的區間，並將其轉換成整數類別。\n",
    "\n",
    "    * 參數：\n",
    "    * n_bins: int or list of ints, 默認為5，指定切割的區間數量或指定每個特徵的切割區間數量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### python\n",
    "\n",
    "<font size = 5> data 調整</font>\n",
    "\n",
    "<font size = 5> StandardScaler </font>\n",
    "\n",
    "StandardScaler：使用StandardScaler对数据进行标准化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardScaler:\n",
      "[[-0.41978194  0.28482986 -1.2879095  ... -1.45900038  0.44105193\n",
      "  -1.0755623 ]\n",
      " [-0.41733926 -0.48772236 -0.59338101 ... -0.30309415  0.44105193\n",
      "  -0.49243937]\n",
      " [-0.41734159 -0.48772236 -0.59338101 ... -0.30309415  0.39642699\n",
      "  -1.2087274 ]\n",
      " ...\n",
      " [-0.41344658 -0.48772236  0.11573841 ...  1.17646583  0.44105193\n",
      "  -0.98304761]\n",
      " [-0.40776407 -0.48772236  0.11573841 ...  1.17646583  0.4032249\n",
      "  -0.86530163]\n",
      " [-0.41500016 -0.48772236  0.11573841 ...  1.17646583  0.44105193\n",
      "  -0.66905833]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder, PolynomialFeatures, KBinsDiscretizer\n",
    "import warnings\n",
    "\n",
    "# 忽略警告訊息\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load the Boston dataset\n",
    "data = load_boston().data\n",
    "\n",
    "# StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "print(\"StandardScaler:\")\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5> RobustScaler </font>\n",
    "\n",
    "RobustScaler：使用RobustScaler对数据进行缩放，可以处理含有异常值的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobustScaler:\n",
      "[[-0.06959315  1.44       -0.57164988 ... -1.33928571  0.26190191\n",
      "  -0.63768116]\n",
      " [-0.06375455  0.         -0.20294345 ... -0.44642857  0.26190191\n",
      "  -0.22188906]\n",
      " [-0.06376011  0.         -0.20294345 ... -0.44642857  0.06667466\n",
      "  -0.73263368]\n",
      " ...\n",
      " [-0.05445006  0.          0.17350891 ...  0.69642857  0.26190191\n",
      "  -0.57171414]\n",
      " [-0.04086745  0.          0.17350891 ...  0.69642857  0.09641444\n",
      "  -0.48775612]\n",
      " [-0.05816351  0.          0.17350891 ...  0.69642857  0.26190191\n",
      "  -0.34782609]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder, PolynomialFeatures, KBinsDiscretizer\n",
    "import warnings\n",
    "\n",
    "# 忽略警告訊息\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Load the Boston dataset\n",
    "data = load_boston().data\n",
    "\n",
    "# RobustScaler\n",
    "robust_scaler = RobustScaler()\n",
    "robust_scaled_data = robust_scaler.fit_transform(data)\n",
    "print(\"RobustScaler:\")\n",
    "print(robust_scaled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5> LabelEncoder </font>\n",
    "\n",
    "LabelEncoder：使用LabelEncoder对一个具有三个以上不同类别的分类变量进行编码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LabelEncoder:\n",
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder, PolynomialFeatures, KBinsDiscretizer\n",
    "\n",
    "# LabelEncoder\n",
    "# Assuming 'data' contains a categorical variable with at least three unique categories\n",
    "le = LabelEncoder()\n",
    "categorical_variable = [\"category1\", \"category2\", \"category3\"]\n",
    "encoded_labels = le.fit_transform(categorical_variable)\n",
    "print(\"LabelEncoder:\")\n",
    "print(encoded_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5> PolynomialFeatures </font>\n",
    "\n",
    "PolynomialFeatures：使用PolynomialFeatures将数据转换为多项式特征。在这个例子中，我们假设原始数据有两个特征(a, b)，转换后会添加额外的特征(a^2, b^2, ab)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PolynomialFeatures:\n",
      "[[6.32000000e-03 1.80000000e+01 2.31000000e+00 ... 1.57529610e+05\n",
      "  1.97656200e+03 2.48004000e+01]\n",
      " [2.73100000e-02 0.00000000e+00 7.07000000e+00 ... 1.57529610e+05\n",
      "  3.62766600e+03 8.35396000e+01]\n",
      " [2.72900000e-02 0.00000000e+00 7.07000000e+00 ... 1.54315409e+05\n",
      "  1.58310490e+03 1.62409000e+01]\n",
      " ...\n",
      " [6.07600000e-02 0.00000000e+00 1.19300000e+01 ... 1.57529610e+05\n",
      "  2.23851600e+03 3.18096000e+01]\n",
      " [1.09590000e-01 0.00000000e+00 1.19300000e+01 ... 1.54802902e+05\n",
      "  2.54955600e+03 4.19904000e+01]\n",
      " [4.74100000e-02 0.00000000e+00 1.19300000e+01 ... 1.57529610e+05\n",
      "  3.12757200e+03 6.20944000e+01]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder, PolynomialFeatures, KBinsDiscretizer\n",
    "import warnings\n",
    "\n",
    "# 忽略警告訊息\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Load the Boston dataset\n",
    "data = load_boston().data\n",
    "\n",
    "# PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "transformed_data = poly.fit_transform(data)\n",
    "print(\"PolynomialFeatures:\")\n",
    "print(transformed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5> KBinsDiscretizer </font>\n",
    "\n",
    "KBinsDiscretizer：使用KBinsDiscretizer将一个具有至少四个不同值的数值变量进行离散化。在这个例子中，我们对第一个特征进行离散化，设定为四个bins，按quantile策略分箱。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KBinsDiscretizer:\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder, PolynomialFeatures, KBinsDiscretizer\n",
    "import warnings\n",
    "\n",
    "# 忽略警告訊息\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Load the Boston dataset\n",
    "data = load_boston().data\n",
    "\n",
    "# KBinsDiscretizer\n",
    "# Assuming 'data' contains a numerical variable with at least four distinct values\n",
    "discretizer = KBinsDiscretizer(n_bins=4, encode='ordinal', strategy='quantile')\n",
    "discretized_data = discretizer.fit_transform(data[:, 0].reshape(-1, 1))\n",
    "print(\"KBinsDiscretizer:\")\n",
    "print(discretized_data[1:20])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
