<!-- markdownlint-disable MD033 -->
<!-- markdownlint-disable MD010 -->
<!-- markdownlint-disable MD029 -->
<!-- markdownlint-disable MD037 -->

# Stacking

Stacking（堆疊法）是一種集成學習方法，旨在<font size = 4 color = orange>通過結合多個基本模型(base model)的預測來提高預測性能</font>。這種方法可以有效地利用不同模型的優勢，並且在一些競賽和實務中經常被使用。

備註 : 基本模型可以是不同類型的機器學習算法，例如回歸模型、分類模型、集成學習模型等。

## 執行步驟

1. 資料準備： 將原始數據分成訓練集和測試集。

2. 第一層（Base Models）： 在第一層中，<font size = 4 color = orange>使用多種不同的基本模型來訓練並得到它們的預測結果</font>。每個基本模型都使用訓練數據進行訓練，然後使用測試數據進行預測。這些<font size = 4 color = orange>基本模型可以是不同種類的機器學習算法</font>，例如決策樹、隨機森林、支持向量機、神經網絡等。

3. 第二層（Meta Model）： 在第二層中，使用上一步得到的基本模型的預測結果作為輸入，並且真實的目標變數（標籤）作為輸出，來訓練一個“元模型”（Meta Model）。這個<font size = 4 color = orange>元模型可以是任何機器學習算法</font>，通常使用較簡單的模型，例如線性回歸、決策樹等。這個元模型的目標是學習如何組合基本模型的預測，以獲得更好的整體性能。

4. 預測： 在測試數據上運行基本模型，獲得每個模型的預測結果。然後，將這些預測結果作為輸入，使用在第二層訓練的元模型進行最終預測。

備註1 : 在第二層訓練 meta model 時, 使用的 data 不能和訓練 base models 時相同, 因為會造成 overfitting.

備註2 : 訓練 meta model 的方法就是把 "validation_x(或稱test_x, 就是不要使用第一層用過的data) 帶入base models" 得到的 pred_y matrix 當作 X, Y 為 validation_y(或稱test_y) 去 fit model. (理論上 validation 和 test 是不同的東西)

[圖片來源](https://www.analyticsvidhya.com/blog/2021/08/ensemble-stacking-for-machine-learning-and-deep-learning/)

![團片](https://editor.analyticsvidhya.com/uploads/39725Stacking.png)

## 優缺點

### 優點

* 提升預測性能： Stacking可以結合多個不同類型的模型，從而充分利用各個模型的優勢，提升整體預測性能。

* 減少過擬合： 通常，基於不同數據和模型的預測結果進行組合，可以減少單一模型的過擬合風險。

* 更靈活的模型組合： Stacking允許在Meta Model中使用不同的加權或非線性組合方式，以提高模型組合的彈性。

* 能處理複雜問題： 當一個問題較為複雜，單個模型難以很好地解決時，Stacking可以將多個模型的能力結合起來，應對複雜問題。

* 自適應調整： 可以根據不同模型的表現調整其權重，讓效果較好的模型具有更大的影響力。

### 缺點

* 複雜性和時間成本： Stacking需要訓練多個Base Models和一個Meta Model，需要額外的時間和計算資源。

* 選擇模型和調參困難： 需要選擇哪些Base Models以及如何組合它們的預測結果，這可能需要一些實驗和調參。

* 風險傳遞： 如果Base Models都犯了相似的錯誤，這些錯誤可能在Meta Model中累積，導致整體效果不佳。

* 過度擬合風險： 如果過度依賴訓練數據進行預測，可能會導致過度擬合。

## 方法

1. 簡單平均（Simple Averaging）

    * 這是最簡單的Stacking方法之一，將不同基本模型的預測結果進行平均。這適用於回歸問題和分類問題。

---

2. 加權平均（Weighted Averaging）

    * 在簡單平均的基礎上，給不同基本模型的預測結果分配不同的權重，根據其表現來調整權重。

---

3. Classic Stacking (Basic Stacking, Blending)：

    * 在這種方法中，訓練集被分成兩部分，第一部分用於訓練基本模型，第二部分用於訓練元模型（通常是一個回歸或分類模型）。
    * 基本模型的預測被用作元模型的輸入，元模型生成最終的預測結果。

---

4. Stacked Generalization (Stacking with Cross-Validation)：

    * 在這種方法中，使用交叉驗證來生成基本模型的預測。每個基本模型在每次交叉驗證中都被訓練和測試，得到完整的訓練集和預測結果。
    * 這些預測結果被用作元模型的特徵，從而生成最終的預測結果。

<!-- 
---
5. Blending：

    * Blending是一種Stacking的變體，其中訓練集被分成兩部分，但基本模型的訓練和測試是在不重疊的子集上進行的。
    * 第一部分用於訓練基本模型，第二部分用於訓練元模型。
    * 基本模型的預測被用作元模型的特徵，元模型生成最終的預測結果。
-->
---

5. Iterative Stacking：

    * 在這種方法中，基本模型的預測被反覆使用來訓練其他基本模型。
    * 首先，訓練一組基本模型，然後使用它們的預測作為新的特徵，再次訓練其他基本模型，以此類推。

---

6. Meta-Features Selection：

    * 在這種方法中，不是所有基本模型的預測都被用作元模型的輸入，而是通過一個選擇過程來選擇最佳的預測特徵。
    * 可以使用特徵選擇方法，比如嵌入式特徵選擇或遞歸特徵消除（Recursive Feature Elimination，RFE），來確定哪些基本模型的預測對元模型的性能影響最大。
