<!-- markdownlint-disable MD033 -->
<!-- markdownlint-disable MD010 -->
<!-- markdownlint-disable MD037 -->

# 集成學習的方法（Ensemble Learning）

1. Bagging（Bootstrap Aggregating）：

   Bagging 通常<font color = orange size = 4>用於降低單個模型的變異性</font>。它通過隨機抽樣（有放回）訓練數據的子集，然後在每個子集上訓練一個基本模型，最後將這些模型的預測結果進行平均（對於回歸）或投票（對於分類）。代表方法包括 Random Forest 和 Extra Trees。

2. Boosting（提升法）：

   Boosting 通常<font color = orange size = 4>用於提高弱模型的性能</font>。它是一種逐步提升模型性能的方法，每個新模型都在之前模型的錯誤上進行訓練。常見的 Boosting 方法有 Adaboost、Gradient Boosting、XGBoost 和 LightGBM。

3. Stacking（堆疊法）：

    Stacking 通常<font color = orange size = 4>將多個基本模型的預測結果作為輸入，訓練一個元模型（也稱為混合模型）來結合這些預測</font>。元模型可以是簡單的線性回歸或其他模型。Stacking 可以充分利用不同基本模型的優勢。

4. Voting（投票法）：

    Voting 是<font color = orange size = 4>將多個基本模型的預測結果進行投票或平均，以決定最終預測結果</font>。Hard Voting 是進行分類的多數決，Soft Voting 是進行回歸的平均。

5. Random Subspace Method：

   Random Subspace Method 是一種 Bagging 的變體，它在每個基本模型訓練時隨機選擇特徵的子集，這有助於提高模型的多樣性。

6. Random Patches Method：

   Random Patches Method 是 Bagging 的另一種變體，它同時對訓練數據和特徵進行隨機抽樣，增加模型的多樣性。

7. Isolation Forest：

    Isolation Forest 是一種使用隨機森林進行異常值檢測的方法，它將異常值視為較少的樣本，並通過隨機劃分特徵空間來構建決策樹，以區分異常值。

8. Bayesian Model Averaging：

    Bayesian Model Averaging 通過使用貝葉斯方法對不同的模型進行權重分配，以得到整體的預測結果。
