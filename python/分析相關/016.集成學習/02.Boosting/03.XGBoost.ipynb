{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "\n",
    "XGBoost 是一種高效的 Gradient Boosting 變體\n",
    "\n",
    "XGBoost 是一種集成學習方法，通過組合多個弱學習器（通常是決策樹）的預測結果，來提高整體模型的準確性。它在 Gradient Boosting 的基礎上進行了一些改進，包括對模型的正則化、自定義損失函數、並引入了稀疏數據的處理方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本步驟\n",
    "\n",
    "1. 初始化模型：初始模型可以是一個常數值，也可以是根據某種啟發式方法設置。\n",
    "\n",
    "2. 計算梯度和二階導數：使用訓練數據計算目標變數的梯度和二階導數，用於指導模型的訓練方向。\n",
    "\n",
    "3. 迭代生成弱學習器：迭代過程中，每次生成一個弱學習器（通常是一棵淺層決策樹）。新的弱學習器將針對之前模型的預測誤差進行擬合。\n",
    "\n",
    "4. 計算步長（learning rate）：根據模型的更新情況計算步長，以控制每個弱學習器的貢獻程度。\n",
    "\n",
    "5. 更新模型：將新生成的弱學習器加入到模型中，並根據步長進行加權。\n",
    "\n",
    "6. 重複迭代：重複執行步驟 3-5，生成多個弱學習器，這些學習器將逐漸減少訓練數據的預測誤差。\n",
    "\n",
    "7. 集成弱學習器：將所有生成的弱學習器組合成一個強大的集成模型，該模型能更好地處理複雜的數據關係。\n",
    "\n",
    "8. 得到最終模型：最終的 XGBoost 模型是多個弱學習器的組合，能夠在預測新數據時具有較高的準確性和泛化能力。\n",
    "\n",
    "### 和 Gradient Boosting 的差別\n",
    "\n",
    "* 正則化策略：XGBoost 在構建模型的過程中使用了正則化，而Gradient Boosting則沒有明確地包含正則化。XGBoost引入了L1（Lasso）和L2（Ridge）正則化，以及樹的深度控制，以避免過度擬合。\n",
    "\n",
    "* 自適應學習率：XGBoost引入了一個自適應學習率（adaptive learning rate）的概念，該學習率在每次迭代中會自動調整，使得模型能更快收斂，同時又不容易出現震盪。\n",
    "\n",
    "* 損失函數的二階導數：XGBoost計算損失函數的一階導數和二階導數，這使得它可以更好地處理非平穩損失函數。\n",
    "\n",
    "* 特徵分裂策略：XGBoost採用了一個基於樣本權重的特徵分裂策略，這可以更好地處理不平衡數據集。\n",
    "\n",
    "* 並行處理：XGBoost能夠利用多核CPU進行並行處理，加速模型的訓練過程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 優缺點\n",
    "\n",
    "### 優點：\n",
    "\n",
    "* 高效性： XGBoost優化了梯度提升算法的效率，使用了並行計算和近似分割等技術，使得訓練速度相對較快。\n",
    "\n",
    "* 正則化： XGBoost允許使用正則化來控制模型的複雜度，防止過擬合。這有助於提高模型的泛化能力。\n",
    "\n",
    "* 支持多種損失函數： XGBoost支持多種損失函數，包括均方誤差（MSE）、二元交叉熵等，使其適用於不同類型的問題。\n",
    "\n",
    "* 處理缺失值： XGBoost能夠自動處理缺失值，不需要預先處理。\n",
    "\n",
    "* 特徵重要性評估： XGBoost可以計算特徵的重要性，幫助了解哪些特徵對於模型的預測最有貢獻。\n",
    "\n",
    "* 靈活性： XGBoost能夠處理各種類型的特徵，包括數值型和類別型。\n",
    "\n",
    "### 限制：\n",
    "\n",
    "* 參數調整： XGBoost的參數相對較多，需要進行複雜的參數調整來達到最佳效果。不恰當的參數設定可能導致過擬合或者欠擬合。\n",
    "\n",
    "* 過度擬合： 如果不適當使用，XGBoost仍然可能面臨過度擬合的問題，特別是在數據量較少的情況下。\n",
    "\n",
    "* 計算資源需求： 由於XGBoost使用了並行計算和多棵樹的集成，可能對計算資源（CPU和內存）的需求較高。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## python\n",
    "\n",
    "```python\n",
    "from xgboost import {model}\n",
    "...\n",
    "```\n",
    "\n",
    "{model} :\n",
    "\n",
    "* XGBClassifier：\n",
    "\n",
    "    XGBClassifier 是 XGBoost 中的分類模型，用於解決分類問題。它是一個集成的強大模型，使用多個弱分類器來預測輸出類別。它的基本思想是通過訓練多個弱分類器，然後將它們的預測結果進行加權組合，以得到最終的預測結果。 XGBClassifier 支持二元分類和多類別分類，並可以自定義目標函數和評估指標。\n",
    "\n",
    "* XGBRegressor：\n",
    "\n",
    "    XGBRegressor 是 XGBoost 中的回歸模型，用於解決回歸問題。與 XGBClassifier 類似，它也是通過集成多個弱回歸器來進行預測。對於回歸任務，XGBRegressor 通常使用均方誤差（MSE）作為目標函數，但也可以自定義其他的目標函數和評估指標。\n",
    "\n",
    "* XGBRanker:\n",
    "\n",
    "    用於排序問題的模型，可以用於搜索引擎的排名、推薦系統等。\n",
    "\n",
    "* XGBRFClassifier 和 XGBRFRegressor:\n",
    "\n",
    "    這是一種基於隨機森林的變體，採用隨機特徵子集進行訓練，以減少過度擬合的風險。\n",
    "\n",
    "* XGBDMatrix:\n",
    "\n",
    "    這是一種特定的數據結構，用於存儲數據和標籤，可以更有效地處理大型數據集。\n",
    "\n",
    "* XGBCallback:\n",
    "\n",
    "    XGBoost 提供了回調函數的功能，可以在訓練過程中添加自定義的回調函數，以實現各種用途，如早期停止、自定義度量等。\n",
    "\n",
    "### 部分參數解釋 :\n",
    "\n",
    "* n_estimators (int): 決定要建立多少個樹模型，即迭代的次數。預設為100。\n",
    "\n",
    "* learning_rate (float): 控制每個弱學習器的權重，也稱為學習率或步長。較小的學習率可以使模型更加穩定，但可能需要更多的迭代次數。通常與n_estimators一起調整，以找到適當的平衡點。預設為0.1。\n",
    "\n",
    "* max_depth (int): 每棵樹的最大深度。深度越大，模型越複雜，容易過擬合。通常與min_child_weight一起調整以避免過擬合。預設為6。\n",
    "\n",
    "* min_child_weight (int): 每個節點的最小樣本數。較小的值可以使模型更傾向於擬合較少數量的樣本，容易過擬合。通常與max_depth一起調整。預設為1。\n",
    "\n",
    "* subsample (float): 控制每個迭代中隨機選取的樣本比例，以減少過擬合。較小的值使得模型更加保守，但也可能減緩學習速度。預設為1，即使用所有樣本。\n",
    "\n",
    "* colsample_bytree (float): 控制每個迭代中隨機選取的特徵比例，以減少過擬合。預設為1，即使用所有特徵。\n",
    "\n",
    "* gamma (float): 控制每個節點分割時所需的最小損失減少。增加此值將導致更保守的模型，避免過度分割。預設為0。\n",
    "\n",
    "* lambda (float): L2 正則化項的權重，用於控制模型的複雜度。增加此值將降低模型的變異性。預設為1。\n",
    "\n",
    "* alpha (float): L1 正則化項的權重，用於進一步控制模型的複雜度。增加此值將降低模型的變異性。預設為0。\n",
    "\n",
    "* objective (str): 定義要最小化的損失函數。常見的目標函數包括'reg:squarederror'（均方誤差）和'binary:logistic'（二元分類的對數損失）。可以根據任務類型選擇合適的目標函數。\n",
    "\n",
    "* eval_metric (str): 衡量模型性能的指標，例如'rmse'（均方根誤差）和'logloss'（對數損失）。通常根據目標函數選擇合適的評估指標。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1 次迭代，訓練集均方誤差: 73.3888\n",
      "第 2 次迭代，訓練集均方誤差: 62.3076\n",
      "第 3 次迭代，訓練集均方誤差: 57.6490\n",
      "第 4 次迭代，訓練集均方誤差: 54.8146\n",
      "第 5 次迭代，訓練集均方誤差: 52.8016\n",
      "第 6 次迭代，訓練集均方誤差: 51.2842\n",
      "第 7 次迭代，訓練集均方誤差: 50.0444\n",
      "第 8 次迭代，訓練集均方誤差: 49.0242\n",
      "第 9 次迭代，訓練集均方誤差: 48.1564\n",
      "第 10 次迭代，訓練集均方誤差: 47.4029\n",
      "第 11 次迭代，訓練集均方誤差: 46.7160\n",
      "第 12 次迭代，訓練集均方誤差: 46.1036\n",
      "第 13 次迭代，訓練集均方誤差: 45.5287\n",
      "第 14 次迭代，訓練集均方誤差: 45.0415\n",
      "第 15 次迭代，訓練集均方誤差: 44.5816\n",
      "第 16 次迭代，訓練集均方誤差: 44.1404\n",
      "第 17 次迭代，訓練集均方誤差: 43.7322\n",
      "第 18 次迭代，訓練集均方誤差: 43.3527\n",
      "第 19 次迭代，訓練集均方誤差: 43.0149\n",
      "第 20 次迭代，訓練集均方誤差: 42.6949\n",
      "第 21 次迭代，訓練集均方誤差: 42.3819\n",
      "第 22 次迭代，訓練集均方誤差: 42.0867\n",
      "第 23 次迭代，訓練集均方誤差: 41.8179\n",
      "第 24 次迭代，訓練集均方誤差: 41.5699\n",
      "第 25 次迭代，訓練集均方誤差: 41.3178\n",
      "第 26 次迭代，訓練集均方誤差: 41.0868\n",
      "第 27 次迭代，訓練集均方誤差: 40.8577\n",
      "第 28 次迭代，訓練集均方誤差: 40.6387\n",
      "第 29 次迭代，訓練集均方誤差: 40.4422\n",
      "第 30 次迭代，訓練集均方誤差: 40.2483\n",
      "第 31 次迭代，訓練集均方誤差: 40.0550\n",
      "第 32 次迭代，訓練集均方誤差: 39.8691\n",
      "第 33 次迭代，訓練集均方誤差: 39.6967\n",
      "第 34 次迭代，訓練集均方誤差: 39.5242\n",
      "第 35 次迭代，訓練集均方誤差: 39.3660\n",
      "第 36 次迭代，訓練集均方誤差: 39.2050\n",
      "第 37 次迭代，訓練集均方誤差: 39.0507\n",
      "第 38 次迭代，訓練集均方誤差: 38.9000\n",
      "第 39 次迭代，訓練集均方誤差: 38.7553\n",
      "第 40 次迭代，訓練集均方誤差: 38.6154\n",
      "第 41 次迭代，訓練集均方誤差: 38.4780\n",
      "第 42 次迭代，訓練集均方誤差: 38.3526\n",
      "第 43 次迭代，訓練集均方誤差: 38.2240\n",
      "第 44 次迭代，訓練集均方誤差: 38.0978\n",
      "第 45 次迭代，訓練集均方誤差: 37.9761\n",
      "第 46 次迭代，訓練集均方誤差: 37.8565\n",
      "第 47 次迭代，訓練集均方誤差: 37.7410\n",
      "第 48 次迭代，訓練集均方誤差: 37.6274\n",
      "第 49 次迭代，訓練集均方誤差: 37.5178\n",
      "第 50 次迭代，訓練集均方誤差: 37.4097\n",
      "第 51 次迭代，訓練集均方誤差: 37.3051\n",
      "第 52 次迭代，訓練集均方誤差: 37.2020\n",
      "第 53 次迭代，訓練集均方誤差: 37.1021\n",
      "第 54 次迭代，訓練集均方誤差: 37.0044\n",
      "第 55 次迭代，訓練集均方誤差: 36.9081\n",
      "第 56 次迭代，訓練集均方誤差: 36.8146\n",
      "第 57 次迭代，訓練集均方誤差: 36.7233\n",
      "第 58 次迭代，訓練集均方誤差: 36.6382\n",
      "第 59 次迭代，訓練集均方誤差: 36.5504\n",
      "第 60 次迭代，訓練集均方誤差: 36.4636\n",
      "第 61 次迭代，訓練集均方誤差: 36.3793\n",
      "第 62 次迭代，訓練集均方誤差: 36.2965\n",
      "第 63 次迭代，訓練集均方誤差: 36.2147\n",
      "第 64 次迭代，訓練集均方誤差: 36.1351\n",
      "第 65 次迭代，訓練集均方誤差: 36.0570\n",
      "第 66 次迭代，訓練集均方誤差: 35.9795\n",
      "第 67 次迭代，訓練集均方誤差: 35.9042\n",
      "第 68 次迭代，訓練集均方誤差: 35.8303\n",
      "第 69 次迭代，訓練集均方誤差: 35.7612\n",
      "第 70 次迭代，訓練集均方誤差: 35.6896\n",
      "第 71 次迭代，訓練集均方誤差: 35.6186\n",
      "第 72 次迭代，訓練集均方誤差: 35.5495\n",
      "第 73 次迭代，訓練集均方誤差: 35.4799\n",
      "第 74 次迭代，訓練集均方誤差: 35.4131\n",
      "第 75 次迭代，訓練集均方誤差: 35.3457\n",
      "第 76 次迭代，訓練集均方誤差: 35.2803\n",
      "第 77 次迭代，訓練集均方誤差: 35.2150\n",
      "第 78 次迭代，訓練集均方誤差: 35.1516\n",
      "第 79 次迭代，訓練集均方誤差: 35.0897\n",
      "第 80 次迭代，訓練集均方誤差: 35.0273\n",
      "第 81 次迭代，訓練集均方誤差: 34.9667\n",
      "第 82 次迭代，訓練集均方誤差: 34.9061\n",
      "第 83 次迭代，訓練集均方誤差: 34.8473\n",
      "第 84 次迭代，訓練集均方誤差: 34.7898\n",
      "第 85 次迭代，訓練集均方誤差: 34.7318\n",
      "第 86 次迭代，訓練集均方誤差: 34.6754\n",
      "第 87 次迭代，訓練集均方誤差: 34.6197\n",
      "第 88 次迭代，訓練集均方誤差: 34.5653\n",
      "第 89 次迭代，訓練集均方誤差: 34.5104\n",
      "第 90 次迭代，訓練集均方誤差: 34.4569\n",
      "第 91 次迭代，訓練集均方誤差: 34.4034\n",
      "第 92 次迭代，訓練集均方誤差: 34.3514\n",
      "第 93 次迭代，訓練集均方誤差: 34.3005\n",
      "第 94 次迭代，訓練集均方誤差: 34.2490\n",
      "第 95 次迭代，訓練集均方誤差: 34.1989\n",
      "第 96 次迭代，訓練集均方誤差: 34.1494\n",
      "第 97 次迭代，訓練集均方誤差: 34.0999\n",
      "第 98 次迭代，訓練集均方誤差: 34.0509\n",
      "第 99 次迭代，訓練集均方誤差: 34.0032\n",
      "第 100 次迭代，訓練集均方誤差: 33.9564\n",
      "測試集均方誤差: 69.92085178801652\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 從 CSV 檔案讀取資料\n",
    "boston_data = pd.read_csv('BostonHousing.csv')\n",
    "\n",
    "# 分割特徵和目標變數\n",
    "X = boston_data.drop('medv', axis=1) \n",
    "y = boston_data['medv']\n",
    "\n",
    "# 分割資料集為訓練集和測試集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 初始化預測結果（初始化為訓練集平均值）\n",
    "y_pred = np.full(y_train.shape, np.mean(y_train))\n",
    "\n",
    "# 設定梯度提升迭代次數和初始學習率\n",
    "n_estimators = 100\n",
    "initial_learning_rate = 0.1\n",
    "\n",
    "# 初始化自適應學習率為初始學習率\n",
    "learning_rate = initial_learning_rate\n",
    "\n",
    "# 建立迭代模型\n",
    "for _ in range(n_estimators):\n",
    "    # 計算殘差\n",
    "    residuals = y_train - y_pred\n",
    "    \n",
    "    # 創建弱學習器（這裡使用決策樹）\n",
    "    base_model = DecisionTreeRegressor(max_depth=3)\n",
    "    base_model.fit(X_train, residuals)\n",
    "    \n",
    "    # 得到弱學習器的預測結果\n",
    "    base_pred = base_model.predict(X_train) # 這裡沒有用正則化當例子\n",
    "    \n",
    "    # 更新預測結果（加上弱學習器預測的部分，乘以學習率）\n",
    "    y_pred += learning_rate * base_pred\n",
    "    \n",
    "    # 計算訓練集的均方誤差\n",
    "    train_mse = mean_squared_error(y_train, y_pred)\n",
    "    \n",
    "    # 更新學習率，可以根據需要進行調整\n",
    "    learning_rate = initial_learning_rate / (1.0 + _)\n",
    "    \n",
    "    print(f\"第 {_+1} 次迭代，訓練集均方誤差: {train_mse:.4f}\")\n",
    "\n",
    "# 計算測試集的預測結果\n",
    "test_pred = np.full(y_test.shape, np.mean(y_train))\n",
    "for _ in range(n_estimators):\n",
    "    test_base_pred = base_model.predict(X_test)\n",
    "    test_pred += learning_rate * test_base_pred\n",
    "\n",
    "# 計算測試集的均方誤差\n",
    "test_mse = mean_squared_error(y_test, test_pred)\n",
    "print(\"測試集均方誤差:\", test_mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "測試集均方誤差: 5.9641193087343005\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 從 CSV 檔案讀取資料\n",
    "boston_data = pd.read_csv('BostonHousing.csv')\n",
    "\n",
    "# 分割特徵和目標變數\n",
    "X = boston_data.drop('medv', axis=1) \n",
    "y = boston_data['medv']\n",
    "\n",
    "# 分割資料集為訓練集和測試集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 創建XGBoost回歸模型\n",
    "model = xgb.XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,  # 控制樹的深度，進行正則化\n",
    "    min_child_weight=1,  # 控制節點的最小樣本數，進行正則化\n",
    "    gamma=0,  # 控制節點分裂的最小損失函數下降，進行正則化\n",
    "    subsample=1,  # 控制每棵樹的隨機樣本比例\n",
    "    colsample_bytree=1,  # 控制每棵樹的隨機特徵比例\n",
    "    reg_alpha=0.5,  # L1 正則化項，加入正則化\n",
    "    reg_lambda=0.5,  # L2 正則化項，加入正則化\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 訓練模型\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 預測測試集\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 計算均方誤差\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"測試集均方誤差:\", mse)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
