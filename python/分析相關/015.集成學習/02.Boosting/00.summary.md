<!-- markdownlint-disable MD033 -->
<!-- markdownlint-disable MD010 -->
<!-- markdownlint-disable MD037 -->

# Boosting

Boosting 是一種集成學習技術，其主要目的是提升弱學習器的性能，使其達到更高的準確性。在 Boosting 中，<font size = 4 color = orange>多個弱學習器（通常是同一種類型的基本模型）被組合在一起，形成一個強大的模型</font>，這個強大的模型能夠處理複雜的學習任務。

## 過程簡述

1. 首先，初始化所有樣本的權重，使其相等。
2. 構建第一個弱學習器，並對樣本進行訓練。
3. 使用第一個弱學習器對訓練樣本進行預測，並根據預測結果調整樣本的權重，對錯誤的樣本增加權重。
4. 構建下一個弱學習器，同樣對權重調整過的樣本進行訓練。
5. 重複步驟3和4，直到構建完所有的弱學習器。
6. 最後，通過組合所有弱學習器的預測結果（例如投票、加權投票或平均）得到最終的預測結果。

在 Boosting 中，<font size = 4 color = orange>每次迭代都是使用同一個基本學習算法，但超參數可以在不同的迭代中進行調整</font>。這是因為 Boosting 的目標是通過集成多個弱學習器來提高模型性能，因此在每次迭代中，可以調整不同的超參數來建立不同的弱學習器，以達到更好的集成效果。

[圖片來源](https://towardsdatascience.com/boosting-algorithms-explained-d38f56ef3f30)
![圖片](https://miro.medium.com/v2/resize:fit:720/format:webp/1*jbncjeM4CfpobEnDO0ZTjw.png)

## 優缺點

### 優點

1. 提升模型性能： Boosting 通常能夠提升模型的預測性能，尤其在弱學習器之間有明顯差異時。

2. 降低過擬合風險： Boosting 通常能夠減少過擬合的風險，因為它在每次迭代中都更關注之前被分類錯誤的樣本，使模型更加專注於學習難以分類的樣本。

3. 可處理高維數據： Boosting 在處理高維數據時表現出色，可以捕捉到複雜的特徵交互關係。

4. 可用於分類和回歸： Boosting 可以用於分類和回歸任務，並且在各種問題上都取得了不錯的成績。

### 缺點

1. 過度依賴難以分類的樣本： Boosting 傾向於過度依賴那些被分類錯誤的樣本，如果這些樣本是噪音，可能導致模型過擬合。

2. 對噪音敏感： Boosting 對於噪音和異常值比較敏感，因為它嘗試在每次迭代中修正之前的錯誤。

3. 計算成本高： Boosting 需要訓練多個弱學習器，並且每次迭代都要計算並更新樣本權重，因此在計算成本上可能相對較高。

## 方法

1. AdaBoost (Adaptive Boosting):

    AdaBoost 是 Boosting 的一個最早和最簡單的形式。它通過反覆訓練一系列弱分類器（通常是 Decision Stumps，即只有一個分割點的短小 Decision Tree），然後根據這些弱分類器的預測結果對錯誤樣本的權重進行調整，以提高預測效果。

2. Gradient Boosting:

    Gradient Boosting 是一種將弱模型組合成強模型的方法。在每次迭代中，它使用梯度下降的思想，使下一個弱模型盡量修正前一次迭代的預測誤差。XGBoost 和 LightGBM 是基於 Gradient Boosting 的優化版本。

3. XGBoost (eXtreme Gradient Boosting):

    XGBoost 是一種高效的 Gradient Boosting 變體，它使用了優化的算法，如權重調整、特徵列分割的合併等，以提高速度和預測性能。它支持回歸、分類和排序問題，並具有許多調優參數。

4. LightGBM:

    LightGBM 是另一種基於 Gradient Boosting 的高效機器學習框架。它的主要特點是在建樹時使用 Leaf-wise 的生長策略，同時具有高效的並行訓練和低記憶體使用。

5. CatBoost:

    CatBoost 是一個開源的梯度提升框架，專為處理類別特徵（Categorical Features）的數據而設計。它能夠自動處理類別特徵的轉換，並在模型訓練過程中進行優化。

6. Histogram-Based Gradient Boosting (HistGradientBoosting):

    這是 Scikit-learn 中的一種 Boosting 方法，特點是使用直方圖加速梯度提升過程，以實現更快的訓練速度。它適用於大數據集，同時保持較高的準確性。

7. LogitBoost:

    LogitBoost 是一種 Boosting 方法，專門設計用於二元分類問題。它通過最小化對數似然損失來調整樣本權重，並利用伯努利分布的特性進行建模。

8. BrownBoost:

    BrownBoost 是 Boosting 方法的一種變體，它根據每個弱分類器的預測誤差對樣本進行權重調整。相對於 AdaBoost，BrownBoost 更強調對難以分類的樣本進行調整，以提高整體性能。

9. LPBoost (Linear Programming Boosting):

    LPBoost 是 Boosting 方法的一種變體，使用線性規劃來解決樣本權重的調整問題。它將 Boosting 轉化為一個線性規劃問題，以提高預測能力。

10. CSBoost (Cost-Sensitive Boosting):

    CSBoost 是一種針對不平衡數據集的 Boosting 方法。它通過設定不同類別的成本權重，以解決類別不平衡導致的問題。

11. LPBoost (Least Probable Class Boosting):

    LPBoost 是一種針對多類別分類的 Boosting 方法，它專注於提高最不可能的類別的預測性能。

12. RUSBoost (Random Under-Sampling Boosting):

    RUSBoost 是一種針對不平衡數據集的 Boosting 方法，通過隨機地下採樣過少類別的樣本來平衡數據分布。

13. SMOTEBoost:

    SMOTEBoost 是一種針對不平衡數據集的 Boosting 方法，通過合成少數類別的合成樣本來平衡數據分布。

14. RGF (Regularized Greedy Forest):

    RGF 是一種基於正則化的 Boosting 方法，它使用樹的結構來進行正則化，以提高模型的泛化能力。
