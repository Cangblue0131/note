<!-- markdownlint-disable MD033 -->
<!-- markdownlint-disable MD010 -->
<!-- markdownlint-disable MD037 -->

# Bagging

Bagging（Bootstrap Aggregating）是一種集成學習方法，它通常使用相同的基本學習算法，但在不同的訓練子集上訓練多個模型，然後將這些模型的預測進行平均或投票來進行最終預測。這有助於減少模型的變異，提高模型的穩定性和泛化能力。

Bagging是一個有效的集成學習方法，特別適用於高變異性模型和降低過度擬合風險。然而，如果模型已經具有足夠的泛化能力，或者需要更好的解釋性，則可能需要考慮其他集成學習方法。

## 優缺點

### 優點

1. 降低過度擬合風險： Bagging可以減少單個模型對訓練數據的過度擬合現象，從而提高模型的泛化能力。

2. 增加穩定性： 通過平均多個模型的預測結果，Bagging可以減少預測的變異性，從而增加整體模型的穩定性。

3. 適用於高變異性模型： Bagging對於高變異性（高方差）的模型特別有效，它可以降低模型的方差，改善模型的性能。

4. 易於實現並行化： Bagging的基本模型之間可以並行訓練和預測，使其易於實現並行計算，提高效率。

### 缺點

1. 不適用於高偏差模型： Bagging對於高偏差（高錯誤）的模型效果有限，因為它不能改善模型的偏差問題。

2. 模型解釋性下降： Bagging集成了多個模型的預測結果，這可能會降低整體模型的可解釋性，因為難以解釋每個模型的貢獻。

3. 可能浪費計算資源： 由於需要訓練多個基本模型，Bagging可能需要更多的計算資源和時間。

## 方法

1. Random Forests：

   Random Forests 是一種基於Bagging的方法，主要應用於決策樹模型。它在每次訓練子集上訓練多個決策樹，同時引入隨機性，包括特徵的隨機選擇和節點劃分時的隨機性，以提高模型的泛化能力。

2. Extra Trees：

    Extra Trees（Extremely Randomized Trees）是另一種基於Bagging的決策樹方法，與Random Forests類似，但更進一步地引入隨機性，不僅在節點劃分時隨機選擇特徵，還隨機選擇劃分的閾值。這樣可以更進一步減少模型的方差。

3. Bagged Decision Trees：

    這是Bagging的基本版本，主要應用於單一決策樹模型。通過在不同的訓練子集上訓練多個決策樹，然後將它們的預測進行平均，從而減少單個決策樹的過擬合，提高整體模型的穩定性。

4. Bagged K-Nearest Neighbors：

    這種方法適用於K-最近鄰居（KNN）算法，通過在不同的訓練子集上訓練多個KNN模型，然後將它們的預測進行平均。這有助於減少KNN模型對局部噪音的過度敏感。
